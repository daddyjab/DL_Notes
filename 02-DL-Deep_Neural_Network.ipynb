{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notes - Implementation of a Deep Neural Network\n",
    "* Author: Jeffery Brown\n",
    "* Topic: Deep Learning\n",
    "* GitHub Repo: https://github.com/daddyjab/DL_Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Summary</font>\n",
    "This Jupyter Notebook provides a Python implementation of a neural network with one or more hidden layers, which was used to classify a set of handwritten digits 0 through 9 as either being or not being a specific single target digit.\n",
    "\n",
    "TBD\n",
    "\n",
    "<!--\n",
    "\n",
    "1. First, key concepts are introduced:\n",
    "\n",
    "    * A representation of a neural network node is introduced and key elements are described\n",
    "    * Forward and Backward Propagation and their role in calculation of predictions and loss/cost (Forward Propagation) and in optimization of model coeffiencients using gradient descent (Backward Propagation) are discussed.\n",
    "    * Python code snippets for performing the calculations is then shown, including use of the `numpy` library for vector and matrix calculations.\n",
    "    \n",
    "2. Next the single node neural network is implemented:\n",
    "\n",
    "    * Dependencies and the MNIST dataset of handwritten digits 0 through 9 are loaded\n",
    "    * Dataset is explored, including checking for missing or invalid values\n",
    "    * The training and testing labels are changed from an integer 0 to 9 matching the handwritten digit image, to a 1 or 0 indicating that the image is (1) or is not (0) a target single digit selected for the analysis, and a helper function is defined and used to plot a sample of digits and their associated labels\n",
    "    * Next, a class `SingleNode()` is defined that comprises the neural network node model, including methods for instantiation, parameter checking, fitting, prediction, and evaluation.  Attributes are defined for store key values such as model coefficients, flags indicating the status of fitting, etc.\n",
    "    * The class is then used to instantiate a model, fit the training data to the model, make predictions using training and testing data, and then evaluate the predictions results.  A helper function is defined and used to plot the model fitting history, which are shown in a figure below.\n",
    "    \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<!-- \n",
    "\n",
    "The single node neural network was used fitted using 60,000 training examples in batches of 32 examples per iteration.  The results for some digits were very good, while the probably of correctly predicting the target digit was lower with other digits.\n",
    "* With `SINGLE_TARGET_DIGIT = 1`, the model performed well, which might be expected since the digit `1` is fairly different in appearance than other digits 0 and 2 through 9.\n",
    "\n",
    "| <b>Table DL-01-A: Single Node Neural Network Performance - Single Target Digit = 1</b> |\n",
    "|:--------------------------------------------------------------------------------------:|\n",
    "\n",
    "|            | Training Data | Testing Data |\n",
    "|:------------|:---------------:|:--------------:|\n",
    "| Count of Examples | 60,000 | 10,000 |\n",
    "| Count of Target Digit in Examples | 6,742 (11.2%) | 1,135 (11.3%) |\n",
    "| Accuracy | 0.9840 | 0.9861 |\n",
    "| Probability of Predicting Correctly<br>when Example is the Target Digit | 0.9058 | 0.9075 |\n",
    "| Probability of Predicting Correctly<br>when Example is Not the Target Digit | 0.9939 | 0.9962 |\n",
    "| Fitting Epochs (Iterations) | 300 | n/a |\n",
    "| Fitting Batch Size | 32 | n/a |\n",
    "| Minimum Batch Loss | 0.0199 @ Epoch 206 | n/a |\n",
    "\n",
    "<br>\n",
    "\n",
    "* However, with `SINGLE_TARGET_DIGIT = 3`, a digit that is more easily confused with other digits, the model performed more poorly.\n",
    "    * The probability of the model correctly predicting the target digit = 3 is low at 0.6525, reflecting the more difficult challenge in distinguishing 3 from digits such as 0 or 8.  The model is bias towards digits being non-target (vs. being the target digit 3).\n",
    "    * The overall accuracy is high, but this is mainly because 88.7% of examples are *not* the target digit 3, and correct predictions of digits as being non-target is good at 0.9945.\n",
    "\n",
    "| <b>Table DL-01-B: Single Node Neural Network Performance - Single Target Digit = 3</b> |\n",
    "|:--------------------------------------------------------------------------------------:|\n",
    "\n",
    "|            | Training Data | Testing Data |\n",
    "|:------------|:---------------:|:--------------:|\n",
    "| Count of Examples | 60,000 | 10,000 |\n",
    "| Count of Target Digit in Examples | 6,131 (10.2% of total) | 1,010 (10.1% of total) |\n",
    "| Accuracy | 0.9581 | 0.9600 |\n",
    "| Probability of Predicting Correctly<br>when Example is the Target Digit | 0.6462 | 0.6525 |\n",
    "| Probability of Predicting Correctly<br>when Example is Not the Target Digit | 0.9936 | 0.9945 |\n",
    "| Fitting Epochs (Iterations) | 300 | n/a |\n",
    "| Fitting Batch Size | 32 | n/a |\n",
    "| Minimum Batch Loss | 0.0560 @ Epoch 128 | n/a |\n",
    "\n",
    "<br><br>\n",
    "* Going foward, the single node  neural network class can be modified to implement more capable neural networks with multiple nodes and layers.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "\n",
    "<!--\n",
    "\n",
    "| Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1 | Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3 |\n",
    "|:----------:|:----------:|\n",
    "| ![Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1...](docs/DL-01-Figure-C-Model_Fit_History-Target_1.png \"Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1\") | ![Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3...](docs/DL-01-Figure-D-Model_Fit_History-Target_3.png \"Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3\") |\n",
    "\n",
    "<br><br>\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "\n",
    "<!--\n",
    "| Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1 |\n",
    "|:----------:|\n",
    "| ![Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1...](docs/DL-01-Figure-C-Model_Fit_History-Target_1.png \"Figure DL-01-C: Model Fit History for a Single Node Neural Network - Single Target Digit = 1\") |\n",
    "\n",
    "<br><br>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "<!--\n",
    "| Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3 |\n",
    "|:----------:|\n",
    "| ![Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3...](docs/DL-01-Figure-D-Model_Fit_History-Target_3.png \"Figure DL-01-D: Model Fit History for a Single Node Neural Network - Single Target Digit = 3\") |\n",
    "\n",
    "<br><br>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> IMPORTANT </font>\n",
    "For introductory notes regarding neural networks, see [01-DL-Single_Node_Neural_Network (Jupyter Notebook)](https://github.com/daddyjab/DL_Notes/blob/master/01-DL-Single_Node_Neural_Network.ipynb), which includes discussion of:\n",
    "* Representation of a Single Neuron (Node)\n",
    "* Forward and Backward Propagation\n",
    "* Activation Functions\n",
    "* Loss Function\n",
    "* Cost Function\n",
    "* Forward Propagation Calculations\n",
    "* Optimization using Gradient Descent\n",
    "* Backward Propagation Calculations\n",
    "* Application of the single node neural network to classification of handwritten digits 0 to 9 as a single target digit or not the target digit.\n",
    "<P>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Representation of Neural Network with Multiple Layers </font>\n",
    "A <b>Neural Network</b> with multiple layers can be represented as shown in Figure DL-02-A: Representation of a Neural Network with Multiple Layers.\n",
    "<P>\n",
    "\n",
    "| Figure DL-02-A: Representation of a Neural Network with Multiple Layers |\n",
    "|:----------:|\n",
    "| ![Figure DL-02-A: Representation of a Neural Network with Multiple Layers is Loading...](docs/DL-02-Figure-A-Neural_Network_Multiple_Layers.png \"Figure DL-02-A: Representation of a Neural Network with Multiple Layers\") |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation Terms:\n",
    "    \n",
    "| Term<br>(for Layer $l$<br>of $L$ total layers) | Description | Definition | Dimensions<br>(rows, columns)|\n",
    "|:----:|:-----------:|:----------------------------------------:|:------------------:|\n",
    "| Input $X$ (or $A^{[0]}$) | Input features of the training or testing examples | Matrix of $n_x$ features for each of $m$ examples | ( $n_x$ features, $m$ examples )\n",
    "| Actual Output $Y$ | Actual labels associated with the training or testing examples | Matrix with $n_y$ outputs for each of $m$ examples | ( $n_y$ outputs, $m$ examples )\n",
    "| Weights $W^{[l]}$ | Weighting coefficients | Matrix of coefficients mapping $n^{[l-1]}$ outputs from the previous layer to $n^{[l]}$ outputs in the current layer| ( $n^{[l]}$ outputs in layer $l$,<br>$n^{[l-1]}$ outputs in layer $l-1$ )\n",
    "| Bias $b^{[l]}$ | Bias coefficients | Vector of coefficients independent of the outputs from the previous layer | ( $n^{[l]}$ units in layer $l$, 1)\n",
    "| Linear function $Z^{[l]}$ | A linear function of the outputs from the previous layer $A^{[l-1]}$ using coefficients $W$ and $b$ | Matrix $Z^{[l]}~=~W^T A^{[l-1]}+b$ | ( $n^{[l]}$ outputs in layer $l$, $m$ examples )\n",
    "| Activation $A^{[l]}$, with:<br>Input $A^{[0]} = X$<br>Predicted Output $A^{[L]} ~=~ \\widehat{Y}$ | Predicted output for the layer based upon output of the layers and an activation function | Matrix $A^{[l]}~=~g^{[l]}(~Z^{[l]}~)$ | ( $n^{[l]}$ outputs in layer $l$, $m$ examples )\n",
    "| Activation Function $g^{[l]}(z)$ | A function which transforms the output of the linear function $z$ and is generally a nonlinear function, such as: sigmoid $\\sigma$, TanH, ReLU, Leaky ReLU, etc. | See [01-DL-Single_Node_Neural_Network (Jupyter Notebook)](https://github.com/daddyjab/DL_Notes/blob/master/01-DL-Single_Node_Neural_Network.ipynb) for the definition of several activation functions $g(z)$ and their derivatives $g^{\\prime}(z)$  | n/a |\n",
    "| Cost Function $J(w,~b)$ | *For a multiple examples*, a measure of the error between the predicted $A^{[L]}$ and actual $Y$ outputs of the neural network | $\\eqalignno {\n",
    "J(w,~b)\n",
    "&= {1 \\over m} \\sum_{i=1}^m \\mathscr{L}(a,~y)\\cr\n",
    "&=- {1 \\over m} \\sum_{i=1}^m \\left [ Y \\log{(A^{[L]}) } + (1-Y) \\log{(1-A^{[L]})} \\right ]\n",
    "}$ | (1, 1) |\n",
    "| Loss Function $\\mathscr{L}(a,~y)$ | *For a single example*, a measure of the error between the predicted $A^{[L]}$ and actual $Y$ outputs of the neural network | $\\mathscr{L}(a,~y)=-(y\\log{(a)}+(1-y)\\log{(1-a)}$ | (1, 1) |\n",
    "| ------------------------------------- | ----------------- | ------------------------------------------------------------------------------| ---------------------------------------------------------- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation Terms:\n",
    "\n",
    "| Term<br>(for Layer $l$<br>of $L$ total layers) | Description | Definition | Dimensions<br>(rows, columns)|\n",
    "|:----:|:-----------:|:----------------------------------------:|:------------------:|\n",
    "| $dA^{[l]}$ | Partial derivative ${ {\\partial \\mathscr{L}(a,~y)} \\over {\\partial a} }$, the change in loss $\\mathscr{L}(a,~y)$ by the change in $A$ | $\\eqalignno {\n",
    "dA^{[L]} &= - { { Y \\over { A^{[L]} } } + { {(1-Y)} \\over { (1-A^{[L]}) } } }\\cr\n",
    "dA^{[l-1]} &= W^{[l]^{T}} dZ^{[l]}\\cr\n",
    "}$ | ( $n^{[l]}$ outputs in layer $l$, $m$ examples )\n",
    "| $dZ^{[l]}$ | Partial derivative ${ {\\partial \\mathscr{L}(a,~y)} \\over {\\partial z} }$, the change in loss $\\mathscr{L}(a,~y)$ by the change in $Z$ | $\\eqalignno {\n",
    "dZ^{[L]} &= A^{[L]} - Y\\cr\n",
    "dZ^{[l-1]} &= dA^{[l-1]} * g^{\\prime [l-1]}( Z^{[l-1]} )\\cr\n",
    "&= W^{[l]^{T}} dZ^{[l]} * g^{\\prime [l-1]}( Z^{[l-1]} )\\cr\n",
    "}$ | ( $n^{[l]}$ outputs in layer $l$, $m$ examples )\n",
    "| $dW^{[l]}$ | Partial derivative ${ {\\partial J(w,~b)} \\over {\\partial W} }$, the change in cost $J(w,~b)$ by the change in $W$.<br>Used to adjust the Weights $W$ during each iteration, scaled by the learning rate $\\alpha$. | $\\eqalignno {\n",
    "dW^{[l]} &= {1 \\over m} dZ^{[l]} A^{[l-1]^{T}}\\cr\n",
    "W^{[l]} &= W^{[l]} - \\alpha ~ dW^{[l]}\n",
    "}$ | ( $n^{[l]}$ outputs in layer $l$,<br>$n^{[l-1]}$ outputs in layer $l-1$ )\n",
    "| $db^{[l]}$ | Partial derivative ${ {\\partial J(w,~b)} \\over {\\partial b} }$, the change in cost $J(w,~b)$ by the change in $b$.<br>Used to adjust the Bias $b$ during each iteration, scaled by the learning rate $\\alpha$. | $\\eqalignno {\n",
    "db^{[l]} &= {1 \\over m} \\sum_{i=1}^m dZ^{[l]}\\cr\n",
    "b^{[l]} &= b^{[l]} - \\alpha ~ db^{[l]}\n",
    "}$ | ( $n^{[l]}$ outputs in layer $l$, 1 )\n",
    "| ------------------------------------- | ----------------- | ------------------------------------------------------------------------------| ---------------------------------------------------------- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "828c2e12-b1c6-4994-8f55-ce86373b6c97"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Jeff's \"standard\" list of libraries to import  - overkill, but keeping them all for convenience... :)\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D    # Support 3D graphing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import table\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# # Machine Learning - Data Preparation and Pre-Processing\n",
    "# from sklearn.model_selection import train_test_split # Split data into training and testing samples\n",
    "# from sklearn.model_selection import cross_val_score  # Score a model using k-fold or other cross validation\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder   # Convert categorical integer features (X) to One-Hot encoded values\n",
    "# from sklearn.preprocessing import LabelEncoder    # Convert categorical labeled values to categorical integer values\n",
    "# from sklearn.preprocessing import LabelBinarizer  # Convert categorical labeled values to Binary encoded values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Scale numerical features to standard normal distribution\n",
    "from sklearn.preprocessing import MinMaxScaler    # Scale numerical values based upon mix/max values\n",
    "\n",
    "# # Machine Learning - Sci-Kit Learn - Models - Regression\n",
    "# from sklearn.linear_model import LinearRegression  # TBD\n",
    "# from sklearn.linear_model import Lasso             # TBD\n",
    "# from sklearn.linear_model import Ridge             # TBD\n",
    "# from sklearn.linear_model import ElasticNet        # TBD\n",
    "\n",
    "# # Machine Learning - Sci-Kit Learn - Models - Classification\n",
    "# from sklearn.linear_model import LogisticRegression   # Logistic Regression Classifier\n",
    "# from sklearn import tree                              # Decision Tree Classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier   # Random Forest Classifier\n",
    "# from sklearn import svm                               # Support Vector Machine Classifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier    # K-Nearest Neighbors (KNN)\n",
    "\n",
    "# # Machine Learning - GridSearch for Hyper-Parameter tuning\n",
    "# from sklearn.model_selection import GridSearchCV      # Grid Search\n",
    "\n",
    "# # Machine Learning - Quantify Model Performance\n",
    "# from sklearn.metrics import mean_squared_error    # Mean Squared Error (MSE) metric\n",
    "# from sklearn.metrics import r2_score              # R-squared (Coefficient of Determination) metric\n",
    "# from sklearn.metrics import confusion_matrix      # Generate a confusion matrix (actual vs. predicted counts)\n",
    "# from sklearn.metrics import classification_report # Calculate metrics for prediction performance\n",
    "# from sklearn.metrics import precision_score       # Calculate the precision: Tp / (Tp + Fp) => Ability to avoid false negatives\n",
    "# from sklearn.metrics import recall_score          # Calculate the recall: Tp / (Tp + Fn) => Ability to find all positive samples\n",
    "# from sklearn.metrics import f1_score              # Calculate the F1 score: 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "# # Machine Learning - Dataset Generation\n",
    "# from sklearn.datasets import make_regression     # Generate linear data\n",
    "# from sklearn.datasets import make_s_curve        # Generate nonlinear data\n",
    "# from sklearn.datasets import make_blobs          # Generate blobs for classification\n",
    "# from sklearn.datasets import make_circles        # Generate circles for classification\n",
    "# from sklearn.datasets import load_iris           # Sample multi-class dataset for classification\n",
    "# from sklearn.datasets import make_classification # Generate datasets for classification\n",
    "\n",
    "# # Machine Learning - Keras (Tensorflow) - Models\n",
    "# from keras.models import Sequential               # Sequential model serving as foundation for neural network\n",
    "# from keras.layers import Dense                    # Nodes for specifying input, hidden, and output layers\n",
    "\n",
    "# # Machine Learning - Keras (Tensorflow) - Encoding\n",
    "# from keras.utils import to_categorical            # One-Hot Encoder provided through Keras\n",
    "\n",
    "# # Machine Learning - Keras (Tensorflow) - Other related Tools\n",
    "# from keras.utils import plot_model                # Plot a neural network model\n",
    "# from keras.models import load_model               # Load a saved machine learning model\n",
    "# from keras.preprocessing import image             # Loads an image for application of machine learning\n",
    "# from keras.preprocessing.image import img_to_array # Converts an image to a numpy array\n",
    "\n",
    "# Machine Learning - Keras (Tensorflow) -  Dataset Generation\n",
    "from keras.datasets import mnist                  # Images: Handwritten digits 0-9 (28x28 grayscale, 60K train, 10K test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a_vector, a_calc_derivative=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the sigmoid activation function\n",
    "    \n",
    "    Arguments:\n",
    "        a_vector: numpy array of values\n",
    "        a_calc_derivative [OPTIONAL]:\n",
    "            False: Calculate the sigmoid function (DEFAULT)\n",
    "            True: Calculate the derivative of the sigmoid function\n",
    "            \n",
    "    Returns:\n",
    "        (See a_calc_derivative above)\n",
    "    \"\"\"\n",
    "\n",
    "    if a_calc_derivative:\n",
    "        return sigmoid( a_vector, False) * (1 - sigmoid(a_vector, False) )\n",
    "    \n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-a_vector) )\n",
    "\n",
    "def relu(a_vector, a_calc_derivative=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the Rectified Linear Unit (ReLU) activation function\n",
    "    \n",
    "    Arguments:\n",
    "        a_vector: numpy array of values\n",
    "        a_calc_derivative [OPTIONAL]:\n",
    "            False: Calculate the ReLU function (DEFAULT)\n",
    "            True: Calculate the derivative of the ReLU function\n",
    "            \n",
    "    Returns:\n",
    "        (See a_calc_derivative above)\n",
    "    \"\"\"\n",
    "\n",
    "    if a_calc_derivative:\n",
    "        return np.where(a_vector > 0, 1, 0)\n",
    "    \n",
    "    else:\n",
    "        return np.maximum(0, a_vector)\n",
    "\n",
    "def leaky_relu(a_vector, a_calc_derivative=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the Leaky Rectified Linear Unit (ReLU) activation function\n",
    "    \n",
    "    Arguments:\n",
    "        a_vector: numpy array of values\n",
    "        a_calc_derivative [OPTIONAL]:\n",
    "            False: Calculate the Leaky ReLU function (DEFAULT)\n",
    "            True: Calculate the derivative of the Leaky ReLU function\n",
    "            \n",
    "    Returns:\n",
    "        (See a_calc_derivative above)\n",
    "    \"\"\"\n",
    "    leak_factor = 0.01\n",
    "    if a_calc_derivative:\n",
    "        return np.where(a_vector > 0, 1, leak_factor)\n",
    "    \n",
    "    else:\n",
    "        return np.maximum(leak_factor*a_vector, a_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Array: [-0.8  0.   0.8]\n",
      "Sigmoid: [0.31002552 0.5        0.68997448], Derivative of Sigmoid: [0.2139097 0.25      0.2139097]\n",
      "ReLU: [0.  0.  0.8], Derivative of ReLU: [0 0 1]\n",
      "Leaky ReLU: [-0.008  0.     0.8  ], Derivative of Leaky ReLU: [0.01 0.01 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# TRYIT!\n",
    "test_array_config = 0.8\n",
    "test_array = np.array([-test_array_config, 0, test_array_config])\n",
    "\n",
    "print(f\"Test Array: {test_array}\")\n",
    "print(f\"Sigmoid: {sigmoid( test_array )}, Derivative of Sigmoid: {sigmoid( test_array, True )}\")\n",
    "print(f\"ReLU: {relu( test_array )}, Derivative of ReLU: {relu( test_array, True )}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu( test_array )}, Derivative of Leaky ReLU: {leaky_relu( test_array, True )}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Evaluate Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Evaluate Prediction Accuracy\n",
    "def evaluate(a_y_predict, a_y_actual):\n",
    "    \"\"\"\n",
    "    Evaluate the model for accuracy\n",
    "\n",
    "    Arguments:\n",
    "        a_y_predict: Predicted output labels\n",
    "        a_y_actual: Actual output labels\n",
    "\n",
    "    Returns: A dictionary with the following values:\n",
    "        'predict_accuracy': Accuracy of the prediction\n",
    "        'predict_proba_label_1': Probability of a correct prediction when actual label is 1\n",
    "        'predict_proba_label_0': Probability of a correct prediction when actual label is 0\n",
    "\n",
    "    Notes:\n",
    "        a_y_predict and a_y_actual must have the same number of elements when flattened\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = a_y_predict.reshape(-1)\n",
    "    y_act = a_y_actual.reshape(-1)\n",
    "    \n",
    "    # Error message if the arguments do not have the same number of elements\n",
    "    assert y_pred.shape == y_act.shape, \\\n",
    "            f\"Arrays y_predict [shape {a_y_predict.shape}] and y_actual [shape {a_y_actual.shape}] must have the same dimensions\"\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    # a_y_predict - a_y_actual will be close to 0 when the prediction is correct,\n",
    "    # and will be close to +/-1 when the prediction is incorrect\n",
    "    # So to calculate the overall accuracy:\n",
    "    # * Take the absolute value of (a_y_predict-a_y_actual)\n",
    "    # * Round the result so that we get either 1.0 or 0.0\n",
    "    # * Sum up the result so that we have a count of incorrect predictions\n",
    "    # * Divide the sum by the number of examples\n",
    "    # * Subtract the result from 1.0 to get the accuracy of the model\n",
    "    predict_accuracy = 1.0 - ( np.sum(np.round(np.abs(y_pred-y_act))) / np.size(y_act) )\n",
    "\n",
    "    # Calculate the probability of predicting 1 given label is 1\n",
    "    # Create a condition array with True where the actual label is 1\n",
    "    cond_label_is_1 = (y_act == 1)\n",
    "\n",
    "    # Apply the condition array to the prediction array to get \n",
    "    # all prediction values where the actual label is 1\n",
    "    y_pred_for_label_1 = np.extract(cond_label_is_1, y_pred)\n",
    "    \n",
    "    # Summing provides a count of the 1 values\n",
    "    predict_proba_label_1 = sum(y_pred_for_label_1) / sum(cond_label_is_1)\n",
    "\n",
    "    # Apply the condition array to the prediction array to get \n",
    "    # all prediction values where the actual label is 0 (i.e., not 1)\n",
    "    y_pred_for_label_0 = np.extract(np.logical_not(cond_label_is_1), y_pred)\n",
    "    \n",
    "    # Summing provides a count of the 1 values / all of the 0 values\n",
    "    predict_proba_label_0 = 1 - (sum(y_pred_for_label_0) / sum(np.logical_not(cond_label_is_1)) )\n",
    "\n",
    "    # Return the accuracy\n",
    "    retval = {\n",
    "        'predict_accuracy': predict_accuracy,\n",
    "        'predict_proba_label_1': predict_proba_label_1,\n",
    "        'predict_proba_label_0': predict_proba_label_0\n",
    "    }\n",
    "    return retval    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.bincount for abc_1 [100 values]: [51 49]\n",
      "np.bincount for abc_2 [100 values]: [48 52]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predict_accuracy': 0.51,\n",
       " 'predict_proba_label_1': 0.5,\n",
       " 'predict_proba_label_0': 0.5208333333333333}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRYIT\n",
    "abc_1 = np.squeeze(np.random.randint( 0, 2, size=(1,100) ))\n",
    "abc_2 = np.squeeze(np.random.randint( 0, 2, size=(1,100) ))\n",
    "print(f\"np.bincount for abc_1 [{len(abc_1)} values]: {np.bincount(abc_1)}\")\n",
    "print(f\"np.bincount for abc_2 [{len(abc_2)} values]: {np.bincount(abc_2)}\")\n",
    "evaluate(abc_1, abc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the class for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_NN():\n",
    "    \"\"\"\n",
    "    class Multilayer_NN\n",
    "    \n",
    "    An implementation of a multilayer neural network.\n",
    "\n",
    "    Public Methods\n",
    "    * configure: Configures the NN\n",
    "    * fit: Trains the NN using specified input feature data and actual output labels\n",
    "    * predict: Predicts output labels based upon specified input feature data\n",
    "    * get_hist: Returns the model fit history\n",
    "    \n",
    "    Method of Use:\n",
    "    1. Instantiate the object: `model = Multilayer_NN()`\n",
    "    2. Configure the model, which also initializes it: model.configure(...)\n",
    "    3. Fit the model to the training data: model.fit(...)\n",
    "    4. Use the model for prediction: model.predict(...)\n",
    "    5. Get useful information about the model: model.get_info(...)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ************************** STANDARD METHODS **************************\n",
    "    def __init__(self, a_layers = None):\n",
    "        \"\"\"\n",
    "        Constructor for the class\n",
    "        \n",
    "        Arguments:\n",
    "            a_layers [OPTIONAL]: A list providing the number of nodes/units in each layer:\n",
    "                        Layer 0: Number of features in input X\n",
    "                        Layers 1 through L-1: Number of nodes/units in hidden layers\n",
    "                        Layer L: Number of outputs in Y\n",
    "   \n",
    "        Returns:\n",
    "            None: Values are initialized in: self._config, self._param, self._cache, self._hist\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize parameters and other key information\n",
    "        self._init_config()\n",
    "        \n",
    "        # If configuration info has been provided, go ahead and configure the model\n",
    "        if a_layers is not None:\n",
    "            self.configure(a_layers)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return an \"official\" string representation of the object\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            String containing an \"official\" representation of the object\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the list of all key/value pairs in the config dictionary\n",
    "        config_list = self._config\n",
    "\n",
    "        # Return the list as a string\n",
    "        retval = f\"{self.__class__.__name__}({config_list})\"        \n",
    "        return retval\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Return an \"friendly\" string representation of the object\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            String containing a \"friendly\" representation of the object\n",
    "        \"\"\"\n",
    "        \n",
    "        # For now, just return whatever is generated from __repr__\n",
    "        retval = self.__repr__()\n",
    "        return retval\n",
    "\n",
    "    \n",
    "    # ************************** PRIVATE METHODS **************************\n",
    "    def _init_config(self):\n",
    "        \"\"\"\n",
    "        Initialize model parameters and other information\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            None: Values are initialized in: self._config, self._param, self._cache, self._hist\n",
    "        \"\"\"\n",
    "        \n",
    "        # Configuration\n",
    "        # - Parameters provided associated with the configuration of the model\n",
    "        #   (provided at configure or fitting)\n",
    "        self._config = {\n",
    "            'is_configured': False,     # Flag: The model has been configured\n",
    "            'is_fitted': False,         # Flag: The model has been fitted to training data\n",
    "            \n",
    "            'alpha': None,              # Learning Rate\n",
    "            'batch_size': None,         # Size of example batches used for validation each iteration\n",
    "            'max_iter': None,           # Maximum number of iteractions to allow\n",
    "            'm': None,                  # Number of examples in the input data\n",
    "            'n_x': None,                # Number of features in the input data\n",
    "            'n_y': None,                # Number of outputs in the output data\n",
    "            'L': None,                  # Total number of hidden layers + output layer\n",
    "            'all_layers': [],           # Layer node/unit counts (i.e., layers 0 through L)\n",
    "        }        \n",
    "        \n",
    "        # Parameters\n",
    "        # - Coeffiecients W and b\n",
    "        self._param = {\n",
    "            'W': {},       # Weight coefficients for each layer 1 through L\n",
    "            'b': {}       # Bias coefficients for each layer 1 through L\n",
    "        }        \n",
    "\n",
    "        # Cache\n",
    "        # - values calculated during forward and backward propagation that need to be retained,\n",
    "        #   including per-layer values of: Z, A, dA, dZ, dW, db\n",
    "        self._cache = {\n",
    "            'Z': {},       # Linear function for each layer 1 through L-1\n",
    "            'A': {},       # Linear function for each layer 1 through L-1, and with A[0] = X, A[L]=Y\n",
    "            'dA': {},      # Partial derivative by A; used to calculate dZ\n",
    "            'dZ': {},      # Partial derivative by Z; used to calculate dW and db\n",
    "            'dW': {},      # Partial derivative by W; used for update of W coefficients\n",
    "            'db': {}       # Partial derivative by b; used for update of b coefficients\n",
    "        }\n",
    "        \n",
    "        # History\n",
    "        # - Fitting history and related info:\n",
    "        self._hist = {\n",
    "            'n_iter': None,          # Number of iterations actually performed\n",
    "            'loss': [],              # List of Cost/Loss by Iteration\n",
    "            'accuracy': []           # List of Per-Batch Validation Accuracy by Iteration\n",
    "        }\n",
    "        \n",
    "    def _init_param(self):\n",
    "        \"\"\"\n",
    "        Initialize model parameters W and b (after the model is configured)\n",
    "\n",
    "        Arguments:\n",
    "            None: Configuration info is obtained from self._config: all_layers, L\n",
    "                        \n",
    "        Returns:\n",
    "            None: Values are initialized for each layer in self._param: W, b\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot initialize parameters W and b until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Make sure the parameter lists are empty\n",
    "        self._param['W'] = {}    # Weight coefficients for each layer 1 through L\n",
    "        self._param['b'] = {}    # Bias coefficients for each layer 1 through L\n",
    "\n",
    "        # Loop through all hidden and the final output layer (i.e., 1 through L)\n",
    "        for layer in range(1, self._config['L']+1):\n",
    "\n",
    "            # Initialize parameters for each layer: Coeffiecients W (to random numbers) and b (to zeros)\n",
    "            W_val = np.random.randn( self._config['all_layers'][layer], self._config['all_layers'][layer-1] ) * 0.01\n",
    "            b_val = np.zeros( (self._config['all_layers'][layer], 1) )\n",
    "            \n",
    "            # Add the parameters to the param dictionary\n",
    "            self._param['W']['W'+str(layer)] = W_val\n",
    "            self._param['b']['b'+str(layer)] = b_val\n",
    "            \n",
    "    def _dump_info(self):\n",
    "        \"\"\"\n",
    "        Basic dump of all internal config, parameter, cache, and history data\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            Dictionary containing internal info: self._config, self._param, self._cache, self._hist\n",
    "        \"\"\"\n",
    "    \n",
    "        # Create a dictionary containing all of the internal data dictionaries\n",
    "        retval = {\n",
    "            '_config': self._config,\n",
    "            '_param': self._param,\n",
    "            '_cache': self._cache,\n",
    "            '_hist': self._hist\n",
    "        }\n",
    "        \n",
    "        return retval\n",
    "\n",
    "    def _set_fit_config(self, a_X, a_y, a_alpha = 0.01, a_batch_size = 32, a_max_iter = 10000):\n",
    "        \"\"\"\n",
    "        Perform fit argument checks and set attributes\n",
    "        \n",
    "        Arguments:\n",
    "            a_X: Input features (numpy array) ~ shape( # features, # examples )\n",
    "            a_y: Output labels (numpy array) ~ shape ( # outputs, # examples )\n",
    "            a_alpha: Learning Rate\n",
    "            a_batch_size: Number of examples to use for validation for each iteration\n",
    "            a_max_iter: Maximum number of iterations to allow\n",
    "            \n",
    "        Returns:\n",
    "            None: Values are updated for each layer in self._config: batch_size, alpha, max_iter, m\n",
    "        \"\"\"\n",
    "\n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot set the fit configuration until model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Confirm a valid batch size has been provided\n",
    "        assert int(a_batch_size) > 0, f\"Batch size must be a number > 0: batch_size = {a_batch_size} [{type(a_batch_size)}]\"\n",
    "        self._config['batch_size'] = int(a_batch_size)   # Batch size to use per iteration\n",
    "        \n",
    "        # Confirm a valid learning rate alpha has been provided\n",
    "        assert float(a_alpha) > 0.0, f\"Learning Rate alpha must be a number > 0.0: alpha = {a_alpha} [{type(a_alpha)}]\"\n",
    "        self._config['alpha'] = float(a_alpha)       # Learning rate alpha to apply\n",
    "                \n",
    "        # Confirm a valid batch size has been provided\n",
    "        assert int(a_max_iter) > 0, f\"Maximum Iterations must be a number > 0: max_iter = {a_max_iter} [{type(a_max_iter)}]\"\n",
    "        self._config['max_iter'] = int(a_max_iter)   # Maximum number of iterations to perform\n",
    "        \n",
    "        # Confirm that the feature count is valid (i.e., matches first entry in all_layers)\n",
    "        assert a_X.shape[0] == self._config['all_layers'][0], f\"Number of features in X must match layers specification: X.shape = {a_X.shape}, all_layers[0] = {self._config['all_layers'][0]}\"\n",
    "        \n",
    "        # Confirm that the output count is valid (i.e., matches last entry in all_layers)\n",
    "        # NOTE: y should be a 2 dimensional numpy array,\n",
    "        #       but because there is usually just 1 output (i.e., 1 row)\n",
    "        #       y *could* be specified with just 1 dimension.\n",
    "        #       => Try to handle this gracefully...\n",
    "        if a_y.ndim > 1:\n",
    "            # y has 2 (or more) dimension, so the 1st dim: # of outputs, 2nd dim: # of examples\n",
    "            assert a_y.shape[0] == self._config['all_layers'][-1], f\"Number of outputs in y must match layers specification: y.shape = {a_y.shape}, all_layers[L={self._config['L']}] = {self._config['all_layers'][-1]}\"\n",
    "            \n",
    "            # Keep track of the number of examples in a_y for a check below\n",
    "            n_y_examples = a_y.shape[1]\n",
    "        \n",
    "        else:\n",
    "            # y has only 1 dimension (should be 2!), so *assume* 1st dim: # of examples and # of outputs == 1\n",
    "            assert self._config['all_layers'][-1] == 1, f\"Number of outputs in y must match layers specification: y.shape = {a_y.shape}, all_layers[L={self._config['L']}] = {self._config['all_layers'][-1]}\"\n",
    "\n",
    "            # Keep track of the number of examples in a_y for a check below\n",
    "            n_y_examples = a_y.shape[0]\n",
    "                       \n",
    "        # Confirm X and Y training data are consistent with configuration\n",
    "        assert a_X.shape[1] == n_y_examples, f\"Number of training examples are different for X vs y: X.shape = {a_X.shape}, y.shape = {a_y.shape}\"\n",
    "        self._config['m'] = int(a_X.shape[1])    # Number of Examples\n",
    "\n",
    "    def _propagate_forward(self, a_X_batch):\n",
    "        \"\"\"\n",
    "        Perform forward propagation calculations using the specified batch examples.\n",
    "        \n",
    "        Arguments:\n",
    "            a_X_batch: A batch subset of input values\n",
    "            \n",
    "        Returns:\n",
    "            A[L]: Predicted output from the output layer\n",
    "        \"\"\"\n",
    "\n",
    "        # DEBUG\n",
    "        DEBUG_LEVEL = 0\n",
    "        \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot propagate forward until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Get the the output layer number\n",
    "        L_val = self._config['L']\n",
    "\n",
    "        # Initialize A0 to X (the input layer)\n",
    "        self._cache['A']['A0'] = a_X_batch\n",
    "        \n",
    "        # Loop through all hidden layers (i.e., 1 through L-1) and the output layer L)\n",
    "        for layer in range(1, L_val+1):\n",
    "\n",
    "            if DEBUG_LEVEL >= 2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_forward(): Starting processing for Layer {layer} of {L_val}\"\n",
    "                print(d_text)\n",
    "\n",
    "            # Ensure that the needed parameter values are present\n",
    "            assert 'W'+str(layer) in self._param['W'].keys(), f\"Missing parameter value W{layer} needed to update layer {layer}: W = {self._param['W']}\"\n",
    "            assert 'b'+str(layer) in self._param['b'].keys(), f\"Missing parameter value b{layer} needed to update layer {layer}: b = {self._param['b']}\"\n",
    "\n",
    "            # Ensure that the needed cache values needed to perform the update are present\n",
    "            assert 'A'+str(layer-1) in self._cache['A'].keys(), f\"Missing cache value A{layer-1} needed to update layer {layer}: A cache = {self._cache['A']}\"\n",
    "\n",
    "            if DEBUG_LEVEL >=2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_forward(): After Validity Checks\\n\"\n",
    "                d_text += f\"W{layer}.shape: {self._param['W']['W'+str(layer)].shape}, \"\n",
    "                d_text += f\"A{layer-1}.shape: {self._cache['A']['A'+str(layer-1)].shape}, \"\n",
    "                print(d_text)\n",
    "            \n",
    "            # Calculate the linear function\n",
    "            Z_val = np.dot(self._param['W']['W'+str(layer)], self._cache['A']['A'+str(layer-1)] ) + self._param['b']['b'+str(layer)]\n",
    "            \n",
    "            if DEBUG_LEVEL >=2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_forward(): Linear Equation (Z)\\n\"\n",
    "                d_text += f\"Z{layer}.shape: {Z_val.shape}, \"\n",
    "                print(d_text)\n",
    "            \n",
    "            # Calculate the activation\n",
    "            if (layer < L_val):\n",
    "\n",
    "                # Use ReLU for all hidden layers 1 through L-1\n",
    "                A_val = relu( Z_val )\n",
    "                \n",
    "            else:\n",
    "                # Use Sigmoid for the output layer L\n",
    "                A_val = sigmoid( Z_val )\n",
    "            \n",
    "            if DEBUG_LEVEL >=2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_forward(): Linear Activaion (A)\\n\"\n",
    "                d_text += f\"A{layer}.shape: {A_val.shape}, \"\n",
    "                print(d_text)\n",
    "            \n",
    "            # Cache the calculated Z and A values\n",
    "            self._cache['Z']['Z'+str(layer)] = Z_val\n",
    "            self._cache['A']['A'+str(layer)] = A_val\n",
    "                \n",
    "        # Return the prediction from the output layer A[L]\n",
    "        return self._cache['A']['A'+str(L_val)]\n",
    "\n",
    "    \n",
    "    def _propagate_backward(self, a_y_batch):\n",
    "        \"\"\"\n",
    "        Perform backward propagation calculations using the specified batch examples.\n",
    "        \n",
    "        Arguments:\n",
    "            a_y_batch: A batch subset of actual output values\n",
    "            \n",
    "        Returns:\n",
    "            None: Values are updated for each layer in self._cache: dA, dZ, dW, db           \n",
    "        \"\"\"\n",
    "\n",
    "        # DEBUG\n",
    "        DEBUG_LEVEL = 0\n",
    "        \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot propagate forward until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Get the the output layer number\n",
    "        L_val = self._config['L']\n",
    "        \n",
    "        # Get the batch size\n",
    "        batch_size_val = self._config['batch_size']\n",
    "\n",
    "        \n",
    "        # Ensure that the needed cache values needed to perform the update are present\n",
    "        assert 'A'+str(L_val) in self._cache['A'].keys(), f\"Missing cache value A{L_val} needed to update layer {layer}: A cache = {self._cache['A']}\"\n",
    "\n",
    "        # Get the predicted output of the output layer: A[L]\n",
    "        AL_val = self._cache['A']['A'+str(L_val)]\n",
    "\n",
    "        # Calculate and cache dA[L] and dZ[L], which require only the predicted and actual output values\n",
    "        self._cache['dA']['dA'+str(L_val)] = - (a_y_batch / AL_val) + (1-a_y_batch) / (1-AL_val)\n",
    "        self._cache['dZ']['dZ'+str(L_val)] = AL_val - a_y_batch\n",
    "\n",
    "        # Loop through all hidden and output layers in reverse order:\n",
    "        # output layer L then hidden layers (i.e., L-1 through 1)\n",
    "        for layer in reversed(range(1, L_val+1)):\n",
    "\n",
    "            if DEBUG_LEVEL >= 2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_backward(): Starting processing for Layer {layer} of {L_val}\"\n",
    "                print(d_text)\n",
    "\n",
    "            # Ensure that the needed parameter values are present\n",
    "            assert 'W'+str(layer) in self._param['W'].keys(), f\"Missing parameter value W{layer} needed to update layer {layer}: W = {self._param['W']}\"\n",
    "\n",
    "            # Retrieve W parameters for (current layer)\n",
    "            W = self._param['W']['W'+str(layer)]\n",
    "            \n",
    "            # Ensure that the needed cache values needed to perform the update are present\n",
    "            assert 'dZ'+str(layer) in self._cache['dZ'].keys(), f\"Missing cache value dZ{layer} needed to update layer {layer}: dZ cache = {self._cache['dZ']}\"\n",
    "            assert 'A'+str(layer-1) in self._cache['A'].keys(), f\"Missing cache value A{layer-1} needed to update layer {layer}: A cache = {self._cache['A']}\"\n",
    "                        \n",
    "            # Retrieve dZ for (current layer) - already cached\n",
    "            dZ = self._cache['dZ']['dZ'+str(layer)]\n",
    "\n",
    "            # Retrive A for (current layer minus 1) - already cached\n",
    "            A_m1 = self._cache['A']['A'+str(layer-1)]\n",
    "\n",
    "            if DEBUG_LEVEL >=2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_backward(): After Retrieving Cached Values\\n\"\n",
    "                d_text += f\"W{layer}.shape: {self._param['W']['W'+str(layer)].shape}, \"\n",
    "                d_text += f\"dZ{layer}.shape: {self._cache['dZ']['dZ'+str(layer)].shape}, \"\n",
    "                d_text += f\"A{layer-1}.shape: {self._cache['A']['A'+str(layer-1)].shape}, \"\n",
    "                print(d_text)\n",
    "            \n",
    "            if layer > 1:\n",
    "                # Ensure that the needed cache values needed to perform the update are present\n",
    "                assert 'Z'+str(layer-1) in self._cache['Z'].keys(), f\"Missing cache value Z{layer-1} needed to update layer {layer}: Z cache = {self._cache['Z']}\"\n",
    "\n",
    "                # Retrive Z for (current layer minus 1) - already cached\n",
    "                Z_m1 = self._cache['Z']['Z'+str(layer-1)]\n",
    "\n",
    "                # Calculate dA for (current layer minus 1): dA_m1\n",
    "                dA_m1 = np.dot(W.T, dZ)\n",
    "                self._cache['dA']['dA'+str(layer-1)] = dA_m1\n",
    "\n",
    "                # Calculate dZ for (current layer minus 1): dZ_m1\n",
    "                dZ_m1 = dA_m1 * relu( Z_m1, a_calc_derivative = True )\n",
    "                self._cache['dZ']['dZ'+str(layer-1)] = dZ_m1\n",
    "                \n",
    "                if DEBUG_LEVEL >=2:\n",
    "                    d_text  = f\"\\nDEBUG: _propagate_backward(): Layer is > 1:\\n\"\n",
    "                    d_text += f\"Z{layer-1}.shape: {self._cache['Z']['Z'+str(layer-1)].shape}, \"\n",
    "                    d_text += f\"dA{layer-1}.shape: {self._cache['dA']['dA'+str(layer-1)].shape}, \"\n",
    "                    d_text += f\"dZ{layer-1}.shape: {self._cache['dZ']['dZ'+str(layer-1)].shape}, \"\n",
    "                    print(d_text)\n",
    "            \n",
    "            # Calculate dW for (current layer): dW\n",
    "            # NOTE: Need to use the batch size, not total number of examples\n",
    "            dW = (1/batch_size_val) * np.dot(dZ, A_m1.T)\n",
    "            self._cache['dW']['dW'+str(layer)] = dW\n",
    "            \n",
    "            # Calculate db for (current layer): db\n",
    "            # NOTE: Need to use the batch size, not total number of examples\n",
    "            db = (1/batch_size_val) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            self._cache['db']['db'+str(layer)] = db\n",
    "            \n",
    "            if DEBUG_LEVEL >=2:\n",
    "                d_text  = f\"\\nDEBUG: _propagate_backward(): After Retrieving Cached Values\\n\"\n",
    "                d_text += f\"dW{layer}.shape: {self._cache['dW']['dW'+str(layer)].shape}, \"\n",
    "                d_text += f\"db{layer}.shape: {self._cache['db']['db'+str(layer)].shape}, \"\n",
    "                print(d_text)\n",
    "\n",
    "    \n",
    "    def _update_param(self):\n",
    "        \"\"\"\n",
    "        Update the coefficients W and b based upon propagation already performed \n",
    "        \n",
    "        Arguments:\n",
    "            None: Needed info is obtained from:\n",
    "                    * self._config: L, alpha\n",
    "                    * self._cache: dW, db\n",
    "\n",
    "        Returns:\n",
    "            None: Values are updated for each layer in self._param: W, b\n",
    "        \"\"\"\n",
    "             \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot update parameters W and b until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Loop through all hidden and the final output layer (i.e., 1 through L)\n",
    "        for layer in range(1, self._config['L']+1):\n",
    "\n",
    "            # Ensure that the W and b parameter values are present\n",
    "            assert 'W'+str(layer) in self._param['W'].keys(), f\"Missing parameter value W{layer} needed to update layer {layer}: W = {self._param['W']}\"\n",
    "            assert 'b'+str(layer) in self._param['b'].keys(), f\"Missing parameter value b{layer} needed to update layer {layer}: b = {self._param['b']}\"\n",
    "\n",
    "            # Ensure that the dW and db cache values needed to perform the update are present\n",
    "            assert 'dW'+str(layer) in self._cache['dW'].keys(), f\"Missing cache value dW{layer} needed to update W{layer}: dW cache = {self._cache['dW']}\"\n",
    "            assert 'db'+str(layer) in self._cache['db'].keys(), f\"Missing cache value db{layer} needed to update b{layer}: db cache = {self._cache['db']}\"\n",
    "                        \n",
    "            # Adjust each parameter by Learning Rate * Derivative for that layer\n",
    "            self._param['W']['W'+str(layer)] -= self._config['alpha'] * self._cache['dW']['dW'+str(layer)]\n",
    "            self._param['b']['b'+str(layer)] -= self._config['alpha'] * self._cache['db']['db'+str(layer)]\n",
    "\n",
    "            \n",
    "\n",
    "    def _calculate_cost(self, a_y_batch):\n",
    "        \"\"\"\n",
    "        Calculate the cost (loss) and accuracy for this iteration and update the fit history\n",
    "        \n",
    "        Arguments:\n",
    "            a_y_batch: A batch subset of actual output values\n",
    "\n",
    "        Returns:\n",
    "            cost_val: Cost(Loss) calculated for this iteration\n",
    "            accuracy_val: Batch accuracy for this iteration\n",
    "        \"\"\"\n",
    "        \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot update parameters W and b until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Get the the output layer number\n",
    "        L_val = self._config['L']\n",
    "        \n",
    "        # Get the batch size\n",
    "        n_batch_size = self._config['batch_size']\n",
    "        \n",
    "        # Get the predicted output of the output layer (i.e., A[L])\n",
    "        AL_val = self._cache['A']['A'+str(L_val)]\n",
    "\n",
    "        # Calculate the cost for this iteration\n",
    "        cost_val = - (1/n_batch_size) * np.sum( a_y_batch*np.log(AL_val) + (1-a_y_batch)*np.log(1-AL_val) )\n",
    "\n",
    "        # Calculate the batch accuracy for this iteration\n",
    "        # NOTE: A-Y = dZ, which is already calculated and cached,\n",
    "        #       but will just use A and Y directly here to remove dependency on back propagation\n",
    "        accuracy_val = 1.0 - ( np.sum(np.round(np.abs(AL_val-a_y_batch))) / n_batch_size )\n",
    "\n",
    "        # Update the fit history with the cost and accuracy for this iteration\n",
    "        self._update_hist(cost_val, accuracy_val)\n",
    "        \n",
    "        return cost_val, accuracy_val\n",
    "\n",
    "\n",
    "    def _update_hist(self, a_cost, a_accuracy ):\n",
    "        \"\"\"\n",
    "        Add one entry to the fit history\n",
    "        \n",
    "        Arguments:\n",
    "            a_cost: Cost(Loss) calculated for this iteration\n",
    "            a_accuracy: Batch accuracy for this iteration\n",
    "            \n",
    "        Returns:\n",
    "            None: Values are updated in self._hist: loss, accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], f\"Cannot update fit history until the model is configured: is_configured = {self._config['is_configured']} [{type(self._config['is_configured'])}]\"\n",
    "\n",
    "        # Add the Cost value J(w,b) to the fit history\n",
    "        self._hist['loss'].append(np.squeeze(a_cost))\n",
    "\n",
    "        # Add the accuracy to the fit history\n",
    "        self._hist['accuracy'].append(np.squeeze(a_accuracy))\n",
    "\n",
    "            \n",
    "    # ************************** PUBLIC METHODS **************************\n",
    "    def configure(self, a_layers = None):\n",
    "        \"\"\"\n",
    "        Configure the number and size of model layers\n",
    "        \n",
    "        Arguments:\n",
    "            a_layers: A list providing the number of nodes/units in each layer:\n",
    "                        Layer 0: Number of features in input X\n",
    "                        Layers 1 through L-1: Number of nodes/units in hidden layers\n",
    "                        Layer L: Number of outputs in Y\n",
    "                        \n",
    "        Returns:\n",
    "            None: Values are updated in:\n",
    "                    * self._config: all_layers, L, n_x, n_y, is_configured\n",
    "                    * self._param: W, b\n",
    "        \"\"\"\n",
    "        \n",
    "        # Confirm that a valid number of hidden layers has been provided\n",
    "        assert isinstance( list(a_layers), list), f\"Must provide a list of node/unit counts for input, hidden, and output layers: layers = {a_layers} [{type(a_layers)}]\"\n",
    "        assert len(list(a_layers)) > 1, f\"Must provide a list of node/unit counts for all input, hidden, and output layers: layers = {a_layers} [{type(a_layers)}]\"\n",
    "        assert min(list(a_layers)) > 0, f\"Each layer must have > 0 nodes/units: layers = {a_layers} [{type(a_layers)}]\"\n",
    "        self._config['all_layers'] = list(a_layers)             # List of all input, hidden, and output layer unit/node counts\n",
    "        self._config['L'] = len(list(a_layers))-1               # Number of hidden + output layers (not incl input layer)\n",
    "        self._config['n_x'] = int(a_layers[0])                  # Number of Input Features (X)\n",
    "        self._config['n_y'] = int(a_layers[self._config['L']])  # Number of Outputs (y)\n",
    "                \n",
    "        # Confirmed that the configuration has been set\n",
    "        self._config['is_configured'] = True;\n",
    "        \n",
    "        # Now that configuration is set,\n",
    "        # initialize the parameters W and b using the configuration information\n",
    "        self._init_param()\n",
    "\n",
    "            \n",
    "    def fit(self, a_X_train, a_y_train, a_alpha = 0.01, a_batch_size = 32, a_max_iter = 10000):\n",
    "        \"\"\"\n",
    "        Fit the model coefficients w and b to the training data with specified arguments\n",
    "        \n",
    "        Arguments:\n",
    "            a_X: Input features (numpy array)\n",
    "            a_y: Output labels (numpy array)\n",
    "            a_alpha: Learning Rate\n",
    "            a_batch_size: Number of examples to use for validation for each iteration\n",
    "            a_max_iter: Maximum number of iterations to allow\n",
    "\n",
    "        Returns:\n",
    "            None: Values are updated in:\n",
    "                    * self._param: W, b\n",
    "                    * self._cache: Z, A, dA, dZ, dW, db\n",
    "                    * self._hist: n_iter, loss, accuracy\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set DEBUG_LEVEL\n",
    "        DEBUG_LEVEL = 1\n",
    "        \n",
    "        # Confirm that the model is configured before attempting to initialize the parameters\n",
    "        assert self._config['is_configured'], \"Cannot fit the model to training data until the model is configured\"\n",
    "\n",
    "        # Set the fit configuration, including basic error checking\n",
    "        self._set_fit_config(a_X_train, a_y_train, a_alpha, a_batch_size, a_max_iter)\n",
    "        \n",
    "        # Do a basic loop through the samples for now\n",
    "        # NEXT: Create a generator function to automatically manage batches\n",
    "        \n",
    "        # Get needed values from config info\n",
    "        batch_size_val = self._config['batch_size']\n",
    "        n_examples_val = self._config['m']\n",
    "        max_iter_val = self._config['max_iter']\n",
    "        \n",
    "        # Calculate the number of iterations available given\n",
    "        # the training sample size and the batch size\n",
    "        n_iter_target = n_examples_val // batch_size_val\n",
    "        \n",
    "        # Limit the number of iterations to max_iter\n",
    "        n_iter_target = min(max_iter_val, n_iter_target)\n",
    "        \n",
    "        # Set the reporting interval at a power of 10 based upon n_iter_target\n",
    "        report_interval = np.power(10,np.round(np.log10(n_iter_target),0)-1)\n",
    "\n",
    "        # Display a progress update\n",
    "        if DEBUG_LEVEL >=2:\n",
    "            d_text  = f\"\\nDEBUG: fit(): After Calculation of Interation Target\\n\"\n",
    "            d_text += f\"n_examples: {n_examples_val}, \"\n",
    "            d_text += f\"batch_size: {batch_size_val}, \"\n",
    "            d_text += f\"max_iter: {max_iter_val}, \"\n",
    "            d_text += f\"n_iter_target: {n_iter_target}, \"            \n",
    "            print(d_text)    \n",
    "\n",
    "        # Initialize fit history\n",
    "        self._hist['n_iter'] = None          # Number of iterations actually performed\n",
    "        self._hist['loss'] = []              # List of Cost/Loss by Iteration\n",
    "        self._hist['accuracy'] = []          # List of Per-Batch Validation Accuracy by Iteration\n",
    "        \n",
    "        # Loop through the iterations\n",
    "        progress_str = \"\"\n",
    "        for i in range(n_iter_target):\n",
    "            \n",
    "            # Get the batch of fit_batch_size examples for this iteration for X and y\n",
    "            # X_batch: a_X_train has shape (examples, features), so transpose to get (features, examples)\n",
    "            # y_batch: a_y_train has shape (examples, 1), so transpose to get (1, examples)\n",
    "            # POSSIBILITY: Could implement batching using numpy masked arrays\n",
    "            # POSSIBILITY: Could implement batching using a generator function\n",
    "            X_batch = a_X_train[:, i*batch_size_val : (i+1)*batch_size_val]\n",
    "            y_batch = a_y_train[:, i*batch_size_val : (i+1)*batch_size_val]\n",
    "            \n",
    "            # Propagate forward to calculate for each layer: Z, A\n",
    "            self._propagate_forward(X_batch)\n",
    "            \n",
    "            # Propagate backward to calculate for each layer: dA, dZ, dW, db\n",
    "            self._propagate_backward(y_batch)\n",
    "            \n",
    "            # Update the parameters for each layer: W, b\n",
    "            self._update_param()\n",
    "            \n",
    "            # Calculate the cost (loss) and accuracy for this iteration\n",
    "            # (including update of the fit history)\n",
    "            cost_val, accuracy_val = self._calculate_cost(y_batch)\n",
    "                        \n",
    "            # Display a progress update periodically (report_interval is a power of 10)\n",
    "            if (DEBUG_LEVEL >= 1) and (i % report_interval == 0):\n",
    "                d_text = f\"[{i}]: Cost J(w,b)={cost_val:0.4f}, Batch Accuracy={accuracy_val:0.4f}\"\n",
    "                print(d_text)\n",
    "                \n",
    "        # Save the number of iterations actually performed\n",
    "        self._hist['n_iter'] = i\n",
    "            \n",
    "        # Set the flag that fit has been performed\n",
    "        self._config['is_fitted'] = True\n",
    "\n",
    "        # Return a string providing the model attributes\n",
    "        # retval = self.__repr__()\n",
    "        # return retval\n",
    "        \n",
    "    def predict(self, a_X_vals):\n",
    "        \"\"\"\n",
    "        Predict the y labels associated with the X feature input\n",
    "        \n",
    "        Arguments:\n",
    "            a_X_vals: Input features (numpy array)\n",
    "            \n",
    "        Returns\n",
    "            A_round: Predicted output (numpy array)\n",
    "        \"\"\"\n",
    "        \n",
    "        # DEBUG\n",
    "        DEBUG_LEVEL = 0\n",
    "        \n",
    "        # Ensure that the model has already been fitted\n",
    "        assert self._config['is_fitted'], f\"The model must be fitted before making predictions: is_fitted = {self._config['is_fitted']}\"\n",
    "\n",
    "        # Use forward propagation to obtain the prediction for the specified feature input\n",
    "        # NOTE: Flag that the forward propagation results should *not* be cached\n",
    "        #       since we're only making a prediction, not training the model\n",
    "        y_predict = self._propagate_forward(a_X_vals)\n",
    "\n",
    "        if DEBUG_LEVEL >=2:\n",
    "            d_text  = f\"\\nDEBUG: predict(): After Prediction\\n\"\n",
    "            d_text += f\"a_X_vals - Shape: {a_X_vals.shape}, Type: {type(a_X_vals)},\\n\"\n",
    "            d_text += f\"y_predict - Shape: {y_predict.shape}, Type: {type(y_predict)},\"\n",
    "            print(d_text)    \n",
    "        \n",
    "        # Round the values \n",
    "        y_predict_rounded = np.round(y_predict).astype(int)\n",
    "        \n",
    "        # Return the prediction\n",
    "        return y_predict_rounded\n",
    "    \n",
    "    \n",
    "    def get_hist(self):\n",
    "        \"\"\"\n",
    "        Returns the fit history\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            Dictionary containing the fit history\n",
    "        \"\"\"\n",
    "    \n",
    "        # Return the dictionary containing fit history\n",
    "        return self._hist\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns configuration information\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "                        \n",
    "        Returns:\n",
    "            Dictionary containing configuration information\n",
    "        \"\"\"\n",
    "    \n",
    "        # Return the dictionary containing fit history\n",
    "        return self._config\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5, 3200), y: (1, 3200)\n"
     ]
    }
   ],
   "source": [
    "# TRYIT\n",
    "all_layers_temp = [5, 2, 1]\n",
    "m_temp = 32*100\n",
    "X_temp = np.random.rand( all_layers_temp[0], m_temp )\n",
    "y_temp = np.random.randint( 0, 2, size = (all_layers_temp[-1], m_temp) )\n",
    "print(f\"X: {X_temp.shape}, y: {y_temp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multilayer_NN({'is_configured': True, 'is_fitted': False, 'alpha': None, 'batch_size': None, 'max_iter': None, 'm': None, 'n_x': 5, 'n_y': 1, 'L': 2, 'all_layers': [5, 2, 1]})"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = Multilayer_NN(all_layers_temp)\n",
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: Cost J(w,b)=0.6931, Batch Accuracy=0.8750\n",
      "[10]: Cost J(w,b)=0.6928, Batch Accuracy=0.5625\n",
      "[20]: Cost J(w,b)=0.6926, Batch Accuracy=0.5625\n",
      "[30]: Cost J(w,b)=0.6933, Batch Accuracy=0.4688\n",
      "[40]: Cost J(w,b)=0.6935, Batch Accuracy=0.4375\n",
      "[50]: Cost J(w,b)=0.6932, Batch Accuracy=0.5000\n",
      "[60]: Cost J(w,b)=0.6928, Batch Accuracy=0.5312\n",
      "[70]: Cost J(w,b)=0.6928, Batch Accuracy=0.5312\n",
      "[80]: Cost J(w,b)=0.6934, Batch Accuracy=0.4688\n",
      "[90]: Cost J(w,b)=0.6918, Batch Accuracy=0.6250\n"
     ]
    }
   ],
   "source": [
    "abc.fit(X_temp, y_temp, a_batch_size = 32, a_max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3200)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_temp = abc.predict(X_temp)\n",
    "y_predict_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict_accuracy': 0.514375,\n",
       " 'predict_proba_label_1': 1.0,\n",
       " 'predict_proba_label_0': 0.0}"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_predict_temp, y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for term_test in ['W', 'b']:\n",
    "#     print(f\"Term '{term_test}':\")\n",
    "#     for k in abc._dump_info()['_param'][term_test].keys():\n",
    "#         print(f\"\\tabc._dump_info()['_param'][term_test]['{k}']: {abc._dump_info()['_param'][term_test][k].shape})\")\n",
    "#         print(f\"{abc._dump_info()['_param'][term_test][k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for term_test in ['A', 'Z', 'dA', 'dZ', 'dW', 'db']:\n",
    "#     print(f\"Term '{term_test}':\")\n",
    "#     for k in abc._dump_info()['_cache'][term_test].keys():\n",
    "#         print(f\"\\tabc._dump_info()['_cache'][term_test]['{k}']: {abc._dump_info()['_cache'][term_test][k].shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_history(a_fit_hist, a_valid_info = None):\n",
    "    \"\"\"\n",
    "    Plot the fit history, including Loss (i.e., Cost) and Training Accuracy if provided\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check fit history arguments\n",
    "    try:\n",
    "        # Get the cost and accuracy lists from the fit history dictionary\n",
    "        h_cost = list(a_fit_hist['cost'])\n",
    "        h_accuracy = list(a_fit_hist['accuracy'])\n",
    "\n",
    "        if len(h_cost) == 0 or len(h_accuracy) == 0 or len(h_cost) != len(h_accuracy):\n",
    "            # Create an error message\n",
    "            err_str  = f\"Error:\\n\"\n",
    "            err_str += f\"\\tNo cost and/or accuracy history provided\\n\"\n",
    "            err_str += f\"\\tCost [{len(h_cost)} values], Accuracy [{len(h_accuracy)} values]\"\n",
    "            raise ValueError(err_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Re-raise the exception that got us here,\n",
    "        # including any error message passed along\n",
    "        raise\n",
    "    \n",
    "    # Check optional validation info arguments, if provided\n",
    "    try:\n",
    "        # Get the overall cost and accuracy scalar values from the validation info dictionary\n",
    "        v_cost = None\n",
    "        v_accuracy = None\n",
    "\n",
    "        if isinstance(a_valid_info,dict) and a_valid_info is not None:\n",
    "            if 'cost' in a_valid_info.keys():\n",
    "                v_cost = float(a_valid_info['cost'])\n",
    "\n",
    "            if 'accuracy' in a_valid_info.keys():\n",
    "                v_accuracy = float(a_valid_info['accuracy'])\n",
    "\n",
    "    except Exception as e:\n",
    "        # Re-raise the exception that got us here,\n",
    "        # including any error message passed along\n",
    "        raise\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Create a single plot of all results\n",
    "    ax1 = fig1.add_subplot( 2,1,1 )\n",
    "    ax2 = fig1.add_subplot( 2,1,2 )\n",
    "\n",
    "    # X-axis - number of epochs in the fit history\n",
    "    x_vals = range(len(h_cost))\n",
    "\n",
    "    # Plots\n",
    "    ax1.set_ylim(ymin=0.9*min(h_accuracy), ymax=1.1)\n",
    "    ax1.plot( x_vals, h_accuracy,\n",
    "              label='Accuracy (Training)', c='k', linestyle='-')\n",
    "\n",
    "    ax2.plot( x_vals, h_cost,\n",
    "              label='Loss (Training)', c='k', linestyle='-')\n",
    "\n",
    "    # Add text note on the max and min accuracy and loss points\n",
    "    acc_min_idx = np.argmin(h_accuracy)\n",
    "    ax1.text( x=acc_min_idx, y=h_accuracy[acc_min_idx]*1.02, c='b',\n",
    "              s=f\"Min: {h_accuracy[acc_min_idx]:.4f}\\nEpoch: {acc_min_idx}\" )\n",
    "\n",
    "    acc_max_idx = np.argmax(h_accuracy)\n",
    "    ax1.text( x=acc_max_idx, y=h_accuracy[acc_max_idx]*1.02, c='b',\n",
    "              s=f\"Max: {h_accuracy[acc_max_idx]:.4f}\\nEpoch: {acc_max_idx}\" )\n",
    "\n",
    "    loss_min_idx = np.argmin(h_cost)\n",
    "    ax2.text( x=loss_min_idx, y=h_cost[loss_min_idx]*1.02, c='r',\n",
    "              s=f\"Min: {h_cost[loss_min_idx]:.4f}\\nEpoch: {loss_min_idx}\" )\n",
    "\n",
    "    loss_max_idx = np.argmax(h_cost)\n",
    "    ax2.text( x=loss_max_idx, y=h_cost[loss_max_idx]*1.02, c='b',\n",
    "              s=f\"Max: {h_cost[loss_max_idx]:.4f}\\nEpoch: {loss_max_idx}\" )\n",
    "    \n",
    "    # Add text note on the accuracy plot at the point when the loss is minimized\n",
    "    ax1.text( x=loss_min_idx, y=h_accuracy[loss_min_idx]*1.02, c='r',\n",
    "              s=f\"Accuracy: {h_accuracy[loss_min_idx]:.4f}\\n@ Min Loss:{h_cost[loss_min_idx]:.4f}\\nEpoch: {loss_min_idx}\" )\n",
    "\n",
    "    # If populated, plot the accuracy from the test samples\n",
    "    try:\n",
    "        if v_accuracy is not None:\n",
    "            # ax1.text( x=min(x_vals), y=v_accuracy*1.02, c='darkgreen', s=f\"{v_accuracy:.4f}\")\n",
    "            ax1.text( x=max(x_vals), y=v_accuracy*1.02, c='darkgreen', s=f\"Testing:\\n{v_accuracy:.4f}\")\n",
    "            ax1.hlines(y=v_accuracy, xmin=min(x_vals), xmax=max(x_vals),\n",
    "                      label='Accuracy (Testing)', color='g', linewidth=3, linestyle=':')    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If populated, plot the Loss (Cost) from the test samples\n",
    "    try:\n",
    "        if v_cost is not None:\n",
    "            ax2.text( x=min(x_vals), y=v_cost*1.02, c='darkgreen', s=f\"{v_cost:.4f}\")\n",
    "            ax2.hlines(y=v_cost, xmin=min(x_vals), xmax=max(x_vals),\n",
    "                      label='Loss (Testing)', color='g', linewidth=3, linestyle=':')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Batch Accuracy\")\n",
    "    ax1.set_title(\"Model Fitting History - Accuracy\")\n",
    "\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Batch Loss\")\n",
    "    ax2.set_title(\"Model Fitting History - Loss\")\n",
    "\n",
    "    fig1.savefig(\"docs/DL-01-Figure-C-Model_Fit_History.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 10   # 10 features\n",
    "n_y = 2    # 1 label with 2 classes\n",
    "n_h = 4    # 4 hidden layers\n",
    "m = 20    # 20 examples\n",
    "\n",
    "Y = np.random.randn( n_y, m )       # Y: shape( n_y, m )\n",
    "\n",
    "X = np.random.randn( n_x, m )       # X: shape( n_x, m )\n",
    "A_0 = X                             # A_0: shape( n_x, m )\n",
    "\n",
    "\n",
    "W_1 = np.random.randn( n_x, n_h )   # W_1: shape( n_h, n_x )  <- Transpose of coursera example\n",
    "b_1 = np.random.randn( n_h, 1 )     # W_1: shape( n_h, 1 )\n",
    "Z_1 = np.dot(W_1.T, A_0) + b_1      # W_1: shape( n_h, m )\n",
    "A_1 = np.tanh(Z_1)                  # W_1: shape( n_h, m )\n",
    "\n",
    "\n",
    "W_2 = np.random.randn( n_h, n_y )   # W_1: shape( n_y, n_h )  <- Transpose of coursera example\n",
    "b_2 = np.random.randn( n_y, 1 )     # W_1: shape( n_y, 1 )\n",
    "Z_2 = np.dot(W_2.T, A_1) + b_2      # W_1: shape( n_y, m )\n",
    "A_2 = np.tanh(Z_2)                  # W_1: shape( n_y, m )\n",
    "\n",
    "print(f\"n_x (# of Inputs): {n_x}\")\n",
    "print(f\"n_y (# of Ouptuts): {n_y}\")\n",
    "print(f\"n_h (# of Hidden Nodes): {n_h}\")\n",
    "print(f\"m (# of Examples): {m}\\n\")\n",
    "\n",
    "print(\"Layer 0 (Input):\")\n",
    "print(f\"X.shape = {X.shape} (features, examples)\")\n",
    "print(f\"A_0.shape = {A_0.shape}\\n\")\n",
    "\n",
    "print(\"Layer 1:\")\n",
    "print(f\"W_1.shape = {W_1.shape}\")\n",
    "print(f\"b_1.shape = {b_1.shape}\")\n",
    "print(f\"Z_1.shape = {Z_1.shape}\")\n",
    "print(f\"A_1.shape = {A_1.shape}\\n\")\n",
    "\n",
    "print(\"Layer 2 (Output):\")\n",
    "print(f\"W_2.shape = {W_2.shape}\")\n",
    "print(f\"b_2.shape = {b_2.shape}\")\n",
    "print(f\"Z_2.shape = {Z_2.shape}\")\n",
    "print(f\"A_2.shape = {A_2.shape}\\n\")\n",
    "\n",
    "print(\"Inputs and Outputs:\")\n",
    "print(f\"Y.shape = {Y.shape} (outputs, examples)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits 0-9 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Plot Lists of Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a list of up to 10 digits on a single subplot\n",
    "def plot_digit_list( a_X_list = None, a_y_list = None, a_find_all_digits = False):\n",
    "    # The first 10 digits from the specified list\n",
    "    \n",
    "    # If no list is specified then return None        \n",
    "    if (a_X_list is None):\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        X_list = list(a_X_list)\n",
    "        \n",
    "    if (a_y_list is None):\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        y_list = list(a_y_list)\n",
    "        \n",
    "    # Find All Digits flag\n",
    "    #   If True => Find and plot all digits 0-9 within the list, starting at index 0\n",
    "    #   If False => Plot up to the first 10 digits in the list\n",
    "    if a_find_all_digits:\n",
    "\n",
    "        # Flag is True: Get indices of samples for each of the digits 0-9 within the 1000 sample subset\n",
    "        # If the digit is not present in the input list then move on to the next digit\n",
    "        d_i_list = []\n",
    "        for d in range(10):\n",
    "            \n",
    "            try:\n",
    "                # Add the index at which this digit can be found to the list\n",
    "                d_i_list.append( y_list.index(d) )\n",
    "                \n",
    "            except ValueError:\n",
    "                # Digit is not present in the input list -- move on to the next digit\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "        # Flag is False: Get the indices for up to the first 10 values in the list\n",
    "        d_i_list = range( min(10, len(y_list) ))\n",
    "    \n",
    "    # The iterpolation method to use for ploting the digit images\n",
    "    i_type_selected = 'lanczos'\n",
    "\n",
    "    print(\"Indices:\", d_i_list)\n",
    "\n",
    "    # Plot Classification Performance results: Best Score vs. Mean Fit Time (ms)\n",
    "    fig = plt.figure(figsize=(20,9))\n",
    "\n",
    "    # Create subplots for each of the sampled digits\n",
    "    for i in range(len(d_i_list)):\n",
    "        # Create a subplot for this iteration\n",
    "        ax = fig.add_subplot( math.ceil(len(d_i_list)/min(5, len(d_i_list))), min(5, len(d_i_list)), i+1 )\n",
    "\n",
    "        # Display a note for each subplot\n",
    "        point_text = f\"Label: {y_list[d_i_list[i]]}\"\n",
    "        point_text += f\"\\nSample Index: {d_i_list[i]}\"\n",
    "    #     ax.text(1, 2+1.4*point_text.count(\"\\n\"), point_text )\n",
    "        ax.set_title(point_text)\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(X_list[d_i_list[i]], cmap=plt.cm.Greys, interpolation=i_type_selected)\n",
    "        \n",
    "    # Return the number of digits plotted\n",
    "    return i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Handwritten Digit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras MNIST handwritten digits sample dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for any missing data values in the training labels\n",
    "sum(y_train == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for imbalance in the training data\n",
    "unique_targets = np.unique(y_train, return_counts=True)\n",
    "unique_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {np.mean(unique_targets[1])}, Median: {np.median(unique_targets[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max: {np.max(unique_targets[1])}, Min: {np.min(unique_targets[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Std Dev: {np.std(unique_targets[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized\n",
    "(unique_targets[1] - np.mean(unique_targets[1]) ) / np.std(unique_targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "# Column (bar) chart\n",
    "ax1.bar(unique_targets[0], unique_targets[1])\n",
    "\n",
    "# Histogram\n",
    "h_retval = ax2.hist(unique_targets[1])\n",
    "\n",
    "# Boxplot\n",
    "b_retval = ax3.boxplot(unique_targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the Labels to a Single Class: Target Digit or Not-Target Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this single neuron exploration,\n",
    "# perform a classification for only one digit as either\n",
    "# the target digit or not the target digit\n",
    "# TARGET_SINGLE_DIGIT = 3\n",
    "TARGET_SINGLE_DIGIT = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing arrays that classify the digit as being either TARGET_SINGLE_DIGIT (1) or not (0)\n",
    "y_train_single = np.array([ (1 if y==TARGET_SINGLE_DIGIT else 0) for y in y_train ])\n",
    "print( f\"Training Data: Count of TARGET_SINGLE_DIGIT [{TARGET_SINGLE_DIGIT}] = {np.sum(y_train_single)} occurences [{np.sum(y_train_single) / np.size(y_train_single):0.1%} of {np.size(y_train_single)} total examples]\")\n",
    "print( f\"Training Data: y_train_single (a few samples): {y_train_single[10:20]}\")\n",
    "print(\"\")\n",
    "\n",
    "y_test_single = np.array([ (1 if y==TARGET_SINGLE_DIGIT else 0) for y in y_test ])\n",
    "print( f\"Testing Data: Count of TARGET_SINGLE_DIGIT [{TARGET_SINGLE_DIGIT}] = {np.sum(y_test_single)} occurences [{np.sum(y_test_single) / np.size(y_test_single):0.1%} of {np.size(y_test_single)} total examples]\")\n",
    "print( f\"Testing Data: y_test_single (a few samples): {y_test_single[10:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to make sure labels are correct\n",
    "check_train_df = pd.DataFrame( { 'y_train': y_train, 'y_train_single': y_train_single }, columns=['y_train', 'y_train_single'])\n",
    "check_test_df = pd.DataFrame( { 'y_test': y_test, 'y_test_single': y_test_single }, columns=['y_test', 'y_test_single'])\n",
    "\n",
    "# Check for occurrences of mismatched labels\n",
    "print( f\"y_train_single: Not the Selected Target Single Digit, but Label == 1 => Occurences { sum( (check_train_df[ 'y_train'] != TARGET_SINGLE_DIGIT) & (check_train_df[ 'y_train_single'] == 1) )}\" )\n",
    "print( f\"y_train_single: Is the Selected Target Single Digit, but Label == 0 => Occurrences { sum( (check_train_df[ 'y_train'] == TARGET_SINGLE_DIGIT) & (check_train_df[ 'y_train_single'] == 0) )}\" )\n",
    "      \n",
    "print( f\"y_test_single: Not the Selected Target Single Digit, but Label == 1 => Occurences { sum( (check_test_df[ 'y_test'] != TARGET_SINGLE_DIGIT) & (check_test_df[ 'y_test_single'] == 1) )}\" )\n",
    "print( f\"y_test_single: Is the Selected Target Single Digit, but Label == 0 => Occurrences { sum( (check_test_df[ 'y_test'] == TARGET_SINGLE_DIGIT) & (check_test_df[ 'y_test_single'] == 0) )}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some samples from the training data\n",
    "plot_digit_list(X_train[10:20], y_train_single[10:20], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some samples from the testing data\n",
    "plot_digit_list(X_test[60:70], y_test_single[60:70], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current shape of the input and output data\n",
    "print(X_train.shape, y_train_single.shape, X_test.shape, y_test_single.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 28x28 pixel images to a 1D array of 784 pixels\n",
    "# and setup the examples in columns (vs. rows)\n",
    "ndims = X_train.shape[1] * X_train.shape[2]\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], ndims)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], ndims)\n",
    "print(X_train_flat.shape, X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a scalar to scale the training data to values between 0 and 1 (MinMaxScalar)\n",
    "# Q: How would the results differ if we applied StandardScalar instead of MixMaxScalar\n",
    "x_scalar = MinMaxScaler().fit(X_train_flat)\n",
    "X_train_scaled = x_scalar.transform(X_train_flat)\n",
    "X_test_scaled = x_scalar.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to apply One-Hot Encoding to the labels\n",
    "# because we have limited the classification to only 2 values (0 or 1)\n",
    "n_classes = 2\n",
    "# y_train_encoded = to_categorical(y_train, n_classes)\n",
    "# y_test_encoded = to_categorical(y_test, n_classes)\n",
    "y_train_encoded = y_train_single.reshape(-1,1)\n",
    "y_test_encoded = y_test_single.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the input and output data\n",
    "print(X_train_scaled.shape, y_train_encoded.shape, X_test_scaled.shape, y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a single node model using the `SingleNode()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the single node model using the SingleNode() class\n",
    "model = SingleNode()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the SingleNode() model fit() method to fit the model to the training data\n",
    "retval = model.fit(X_train_scaled, y_train_encoded, 0.05, 32, 300)\n",
    "print(retval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions using the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the training data\n",
    "y_act_train = y_train_encoded[:]\n",
    "y_pred_train = model.predict(X_train_scaled[:,:])\n",
    "\n",
    "# Evaluate the performance of the model based upon the training data\n",
    "retval_train = model.evaluate(y_pred_train, y_act_train )\n",
    "print(f\"Prediction Performance using Training Data ({np.size(y_act_train)} examples):\")\n",
    "print(f\"\\tAccuracy: {retval_train['predict_accuracy']:0.4f}\")\n",
    "print(f\"\\tProbability of Predicting Correctly when Actual Label is Target Digit: {retval_train['predict_proba_label_1']:0.4f}\")\n",
    "print(f\"\\tProbability of Predicting Correctly when Actual Label is Not the Target Digit: {retval_train['predict_proba_label_0']:0.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Make predictions using the testing data\n",
    "y_act_test = y_test_encoded[:]\n",
    "y_pred_test = model.predict(X_test_scaled[:,:])\n",
    "\n",
    "# Evaluate the performance of the model based upon the training data\n",
    "retval_test = model.evaluate(y_pred_test, y_act_test )\n",
    "print(f\"Prediction Performance using Testing Data ({np.size(y_act_test)} examples):\")\n",
    "print(f\"\\tAccuracy: {retval_test['predict_accuracy']:0.4f}\")\n",
    "print(f\"\\tProbability of Predicting Correctly when Actual Label is Target Digit: {retval_test['predict_proba_label_1']:0.4f}\")\n",
    "print(f\"\\tProbability of Predicting Correctly when Actual Label is Not the Target Digit: {retval_test['predict_proba_label_0']:0.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Model Fitting History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit_history(model.fit_history, {'accuracy': retval_test['predict_accuracy']} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbpresent": {
   "slides": {
    "03210a56-863e-4749-b7ba-ed75bfceceee": {
     "id": "03210a56-863e-4749-b7ba-ed75bfceceee",
     "prev": "86b3b05f-6e9a-49dc-8a83-97f72c348c5f",
     "regions": {
      "2be9481c-cff7-4f32-b835-90f2a2cb989a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "46550b81-7dd8-4efc-b710-0f10002c9f2b",
        "part": "whole"
       },
       "id": "2be9481c-cff7-4f32-b835-90f2a2cb989a"
      },
      "3f484569-30ec-4529-8d3c-ac88b8c6dbfb": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "2b83c32a-d7e2-4db6-b45f-afbbf028fe90",
        "part": "whole"
       },
       "id": "3f484569-30ec-4529-8d3c-ac88b8c6dbfb"
      }
     }
    },
    "22531930-3fc8-45ff-a4f1-32e94c1d1455": {
     "id": "22531930-3fc8-45ff-a4f1-32e94c1d1455",
     "prev": "8eb735f6-11af-4f77-a4ed-b637fb18ac08",
     "regions": {
      "1407c818-c9fd-470e-8260-6b6904d888de": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "664c64ef-5517-4725-95cb-874bbc7711c7",
        "part": "whole"
       },
       "id": "1407c818-c9fd-470e-8260-6b6904d888de"
      }
     }
    },
    "29c55765-6994-4dbf-b82f-117b4f1e0cee": {
     "id": "29c55765-6994-4dbf-b82f-117b4f1e0cee",
     "prev": "b7c25bc1-4dd7-421b-9951-0c0c403c176d",
     "regions": {
      "44583f15-ec4f-4aec-8e7c-f01652e954fc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7c23961d-86fe-4d07-bc58-6d61f03c3731",
        "part": "whole"
       },
       "id": "44583f15-ec4f-4aec-8e7c-f01652e954fc"
      }
     }
    },
    "39c5764b-ab2c-4aae-8500-dc39319e4d38": {
     "id": "39c5764b-ab2c-4aae-8500-dc39319e4d38",
     "prev": "22531930-3fc8-45ff-a4f1-32e94c1d1455",
     "regions": {
      "d0237596-8095-44dc-81c6-6d9c6b204684": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "348e164e-a000-4def-92c0-db93f9a617a1",
        "part": "whole"
       },
       "id": "d0237596-8095-44dc-81c6-6d9c6b204684"
      }
     }
    },
    "39e950f1-f2c4-422a-8f45-835c5c61e7e0": {
     "id": "39e950f1-f2c4-422a-8f45-835c5c61e7e0",
     "prev": "4b5f1824-db4a-4ef3-9515-a88d3f2276c1",
     "regions": {
      "0a72cc48-c38f-4e8c-94d3-3198eccde68a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3820b22-bf55-4dca-ba8b-f08060567b6e",
        "part": "whole"
       },
       "id": "0a72cc48-c38f-4e8c-94d3-3198eccde68a"
      },
      "b49c6eca-318f-41b8-a7a5-a620d18f639c": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "aeeeb44d-7825-4050-b7ba-70813bd3396b",
        "part": "whole"
       },
       "id": "b49c6eca-318f-41b8-a7a5-a620d18f639c"
      }
     }
    },
    "4792c0cc-ccd4-48bf-9b0e-a3c1f0f3008d": {
     "id": "4792c0cc-ccd4-48bf-9b0e-a3c1f0f3008d",
     "prev": "39c5764b-ab2c-4aae-8500-dc39319e4d38",
     "regions": {
      "d4adae26-ff2f-4a81-ad91-fd3a9bfd965c": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "4d6b8af6-438e-4034-bda1-1f977cf12441",
        "part": "whole"
       },
       "id": "d4adae26-ff2f-4a81-ad91-fd3a9bfd965c"
      },
      "d75a8261-ee89-4f4d-b388-2f429e112f44": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a3b6e99c-2c85-47da-b29b-3b53c3c309b8",
        "part": "whole"
       },
       "id": "d75a8261-ee89-4f4d-b388-2f429e112f44"
      }
     }
    },
    "4b5f1824-db4a-4ef3-9515-a88d3f2276c1": {
     "id": "4b5f1824-db4a-4ef3-9515-a88d3f2276c1",
     "prev": "e92a9020-6631-400a-aa50-46c1fa0a5f0c",
     "regions": {
      "2bd81133-11e3-48df-bf12-2d0a57949bca": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "69fab88f-0864-47cc-933b-eaffb638cbf3",
        "part": "whole"
       },
       "id": "2bd81133-11e3-48df-bf12-2d0a57949bca"
      },
      "d3322861-28ab-4f1e-8a1f-3a7f4d3da8c1": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "3810c2af-fdbe-4f01-bc9f-2bb1a8ae70c6",
        "part": "whole"
       },
       "id": "d3322861-28ab-4f1e-8a1f-3a7f4d3da8c1"
      }
     }
    },
    "57992fb9-2f74-4e8e-8772-7b9f04606a56": {
     "id": "57992fb9-2f74-4e8e-8772-7b9f04606a56",
     "prev": "595093b4-3f86-406c-abb5-aea78e1241b3",
     "regions": {
      "3b3c8191-9f7e-4ab2-afac-b2b47008484a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a6d0bdff-9bba-472e-b354-da47b8761dbe",
        "part": "whole"
       },
       "id": "3b3c8191-9f7e-4ab2-afac-b2b47008484a"
      }
     }
    },
    "5907f515-df93-4e86-82c6-b8721b6e6346": {
     "id": "5907f515-df93-4e86-82c6-b8721b6e6346",
     "prev": "39e950f1-f2c4-422a-8f45-835c5c61e7e0",
     "regions": {
      "22f9005a-ed7a-4399-b501-497565ada2da": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2e255efc-6ae1-4554-b32f-9400b47007c9",
        "part": "whole"
       },
       "id": "22f9005a-ed7a-4399-b501-497565ada2da"
      },
      "82d64cef-48f7-413c-ad01-e7fa2921f7c0": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "711a40c8-c479-448b-a3bc-5c7ff8369838",
        "part": "whole"
       },
       "id": "82d64cef-48f7-413c-ad01-e7fa2921f7c0"
      },
      "d943d3ea-6cd3-4d4a-8914-7fea0231efda": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "9bfbb27b-cae0-4feb-809a-77718bb2e6a3",
        "part": "whole"
       },
       "id": "d943d3ea-6cd3-4d4a-8914-7fea0231efda"
      }
     }
    },
    "595093b4-3f86-406c-abb5-aea78e1241b3": {
     "id": "595093b4-3f86-406c-abb5-aea78e1241b3",
     "prev": "5907f515-df93-4e86-82c6-b8721b6e6346",
     "regions": {
      "3366a1dc-6c33-45f0-a97f-a2744ed0587f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e54b79a1-316d-4fd1-ad5f-6cd580b51f99",
        "part": "whole"
       },
       "id": "3366a1dc-6c33-45f0-a97f-a2744ed0587f"
      },
      "40b6dba8-5994-4fd0-9ce5-9d36f3bb9f93": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "3d860568-b048-40f2-a64d-f585211d6c1c",
        "part": "whole"
       },
       "id": "40b6dba8-5994-4fd0-9ce5-9d36f3bb9f93"
      }
     }
    },
    "76838eab-c2c3-4ff3-b09f-854a7c74c6d4": {
     "id": "76838eab-c2c3-4ff3-b09f-854a7c74c6d4",
     "prev": null,
     "regions": {
      "b0032c00-1d89-4c94-a7a0-488f243c381e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0dc0cdcb-d8df-4a29-a0d2-a3cf6cc889ce",
        "part": "whole"
       },
       "id": "b0032c00-1d89-4c94-a7a0-488f243c381e"
      }
     }
    },
    "7a25817a-e5bc-4021-9c6b-61bc74dbdb92": {
     "id": "7a25817a-e5bc-4021-9c6b-61bc74dbdb92",
     "prev": "b04719fc-934a-404b-a642-dc732a3d1589",
     "regions": {
      "38d56409-500a-45cf-8e79-722d68d8fc38": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "335f050d-99af-43b3-a5ec-69fe896e48e2",
        "part": "whole"
       },
       "id": "38d56409-500a-45cf-8e79-722d68d8fc38"
      },
      "c838cb6d-409c-4c17-a1eb-3d7dc6aab32b": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "19ca6a7e-4b5d-4b0f-88c5-e68634abb65d",
        "part": "whole"
       },
       "id": "c838cb6d-409c-4c17-a1eb-3d7dc6aab32b"
      }
     }
    },
    "86b3b05f-6e9a-49dc-8a83-97f72c348c5f": {
     "id": "86b3b05f-6e9a-49dc-8a83-97f72c348c5f",
     "prev": "57992fb9-2f74-4e8e-8772-7b9f04606a56",
     "regions": {
      "49a41ef6-224d-46c1-bb81-254b8a4cee88": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d7df6fa0-ce96-4073-b95d-ce78b3f4fdf1",
        "part": "whole"
       },
       "id": "49a41ef6-224d-46c1-bb81-254b8a4cee88"
      },
      "7ce316ab-4799-4e15-b095-374da21bc1aa": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "4a5226b5-d431-4cbe-86a6-291952a45a37",
        "part": "whole"
       },
       "id": "7ce316ab-4799-4e15-b095-374da21bc1aa"
      },
      "a7d701c6-61e3-48e3-8e0d-a4b318aef7d8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "5e425283-82de-4c48-b2ae-31a1d6983849",
        "part": "whole"
       },
       "id": "a7d701c6-61e3-48e3-8e0d-a4b318aef7d8"
      },
      "e180cf23-71a7-46a8-8598-86a079bd7d78": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "28aaab45-1e14-4db5-9d2c-63b239cafe13",
        "part": "whole"
       },
       "id": "e180cf23-71a7-46a8-8598-86a079bd7d78"
      }
     }
    },
    "8eb735f6-11af-4f77-a4ed-b637fb18ac08": {
     "id": "8eb735f6-11af-4f77-a4ed-b637fb18ac08",
     "prev": "7a25817a-e5bc-4021-9c6b-61bc74dbdb92",
     "regions": {
      "ca38ef02-01f9-42cb-9e35-d67f8d665597": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "69b638ed-ed32-4824-bb81-df958c215a33",
        "part": "whole"
       },
       "id": "ca38ef02-01f9-42cb-9e35-d67f8d665597"
      },
      "fc52547b-a8f8-40e2-9397-5b18ca2b71b3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7317e4f2-150e-4336-ac48-d82641441aa8",
        "part": "whole"
       },
       "id": "fc52547b-a8f8-40e2-9397-5b18ca2b71b3"
      }
     }
    },
    "af2f5933-b31d-4d30-b08a-e4c09432b9bb": {
     "id": "af2f5933-b31d-4d30-b08a-e4c09432b9bb",
     "prev": "29c55765-6994-4dbf-b82f-117b4f1e0cee",
     "regions": {
      "479b436f-50bb-4c3a-9a4e-748e85eacde6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6d53481c-69ff-41a0-937e-4e8a39e2b2b5",
        "part": "whole"
       },
       "id": "479b436f-50bb-4c3a-9a4e-748e85eacde6"
      },
      "a02a5bdc-cab5-4ddc-aa9d-8ee5e0db6bd4": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "b0f8400f-71e1-4892-9012-88e54c053a36",
        "part": "whole"
       },
       "id": "a02a5bdc-cab5-4ddc-aa9d-8ee5e0db6bd4"
      }
     }
    },
    "b04719fc-934a-404b-a642-dc732a3d1589": {
     "id": "b04719fc-934a-404b-a642-dc732a3d1589",
     "prev": "af2f5933-b31d-4d30-b08a-e4c09432b9bb",
     "regions": {
      "701cdc94-3399-40dc-8e45-10d6edbf0959": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "fa78fa75-54cf-40d3-b52c-4b51c5070d9a",
        "part": "whole"
       },
       "id": "701cdc94-3399-40dc-8e45-10d6edbf0959"
      },
      "81f01069-a0c6-4ab1-98d1-d465e96aac6d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fb58a52c-71e9-402d-af86-eab194cf8050",
        "part": "whole"
       },
       "id": "81f01069-a0c6-4ab1-98d1-d465e96aac6d"
      },
      "ae73fc9b-89cf-4405-ab54-34e6c8b73a19": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "241f356d-567a-421d-b27c-7ce2d146d1bb",
        "part": "whole"
       },
       "id": "ae73fc9b-89cf-4405-ab54-34e6c8b73a19"
      }
     }
    },
    "b7c25bc1-4dd7-421b-9951-0c0c403c176d": {
     "id": "b7c25bc1-4dd7-421b-9951-0c0c403c176d",
     "prev": "76838eab-c2c3-4ff3-b09f-854a7c74c6d4",
     "regions": {
      "47f524f5-68f8-49df-9a61-61eab6cab03c": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "966e050d-5739-4ef9-a2ae-9a4957e14f3e",
        "part": "whole"
       },
       "id": "47f524f5-68f8-49df-9a61-61eab6cab03c"
      },
      "4b3ba01a-29e7-4e67-ac78-988162f7dde9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "91401f60-44eb-42b2-86ea-c82b5e955ace",
        "part": "whole"
       },
       "id": "4b3ba01a-29e7-4e67-ac78-988162f7dde9"
      },
      "a47d9662-0a49-4b84-aae0-908067552ebd": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "6de4dae8-c949-4927-89a3-9741aba83e8d",
        "part": "whole"
       },
       "id": "a47d9662-0a49-4b84-aae0-908067552ebd"
      },
      "e64668fe-aafe-4e24-a46d-6a6266eeee77": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "7068c96d-f8f0-42bb-bdfd-dcbf9335323c",
        "part": "whole"
       },
       "id": "e64668fe-aafe-4e24-a46d-6a6266eeee77"
      }
     }
    },
    "d3ecec0a-67fe-4558-a268-50695fd7fe1c": {
     "id": "d3ecec0a-67fe-4558-a268-50695fd7fe1c",
     "prev": "fcda7de6-cb93-4b63-9dea-d5f23fecd968",
     "regions": {
      "b268bb57-2543-4b91-b9c0-84aa3cf09d5c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67aab8fa-45cd-4ede-8285-e3dad22d8b16",
        "part": "whole"
       },
       "id": "b268bb57-2543-4b91-b9c0-84aa3cf09d5c"
      }
     }
    },
    "e92a9020-6631-400a-aa50-46c1fa0a5f0c": {
     "id": "e92a9020-6631-400a-aa50-46c1fa0a5f0c",
     "prev": "ee94ae9b-9e13-414c-a017-db1d8913aaa8",
     "regions": {
      "bb15b4ea-132f-4146-ab77-93a4969c2904": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7d5d2002-5cae-4d5f-8e2e-d5eaf5cd17da",
        "part": "whole"
       },
       "id": "bb15b4ea-132f-4146-ab77-93a4969c2904"
      }
     }
    },
    "ee94ae9b-9e13-414c-a017-db1d8913aaa8": {
     "id": "ee94ae9b-9e13-414c-a017-db1d8913aaa8",
     "prev": "f8fce519-927e-4fd0-88d8-23ece4403794",
     "regions": {
      "0b578bb1-2cd1-480b-8d65-dde0861851cb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f1c699bc-6717-4eb9-8e6d-66d433ba4618",
        "part": "whole"
       },
       "id": "0b578bb1-2cd1-480b-8d65-dde0861851cb"
      }
     }
    },
    "f8fce519-927e-4fd0-88d8-23ece4403794": {
     "id": "f8fce519-927e-4fd0-88d8-23ece4403794",
     "prev": "4792c0cc-ccd4-48bf-9b0e-a3c1f0f3008d",
     "regions": {
      "7510ae01-fe22-48b0-92cb-29b880cc85fc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e61ba538-5828-4a5a-a2c5-4fcceac35889",
        "part": "whole"
       },
       "id": "7510ae01-fe22-48b0-92cb-29b880cc85fc"
      }
     }
    },
    "fcda7de6-cb93-4b63-9dea-d5f23fecd968": {
     "id": "fcda7de6-cb93-4b63-9dea-d5f23fecd968",
     "prev": "03210a56-863e-4749-b7ba-ed75bfceceee",
     "regions": {
      "43f5b3bd-9e0c-4a98-a18c-1b9f7beaa3b8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "318099ba-93dd-4c2c-a3ee-fc6622c472c8",
        "part": "whole"
       },
       "id": "43f5b3bd-9e0c-4a98-a18c-1b9f7beaa3b8"
      },
      "dbab9920-f08a-457a-949d-2c8289707798": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "8bb35ef7-00cc-4ad9-9dd4-6fac480df38e",
        "part": "whole"
       },
       "id": "dbab9920-f08a-457a-949d-2c8289707798"
      }
     }
    }
   },
   "themes": {}
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
